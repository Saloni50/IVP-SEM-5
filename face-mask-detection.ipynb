{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FACE-MASK-DETECTION\n\nWe are using Face Mask Detection dataset by Larxel (andrewmvd) from Kaggle.\n* Dataset link: https://www.kaggle.com/andrewmvd/face-mask-detection\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Install and Import Libraries","metadata":{}},{"cell_type":"code","source":"!pip install opencv-python","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Data Manipulation\nimport pandas as pd\n\n# Numerical Analysis\nimport numpy as np\n\n# Data Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Operating System\nimport os\n\n# Deep Learning and Object Detection\nimport tensorflow as tf\nfrom tensorflow import keras\nimport cv2\n\n# Data Extraction\nimport glob\nfrom xml.etree import ElementTree","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('Tensorflow Version: {}'.format(tf.__version__))\nprint('Keras Version: {}'.format(keras.__version__))","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"annotations_directory = '../input/face-mask-detection/annotations'\nimages_directory = '../input/face-mask-detection/images'","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"annotations_files = !ls '../input/face-mask-detection/annotations'\nannotations_files[:10]","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"images_files = !ls '../input/face-mask-detection/images'\nimages_files[:10]","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(annotations_files), len(images_files)","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Data Extraction","metadata":{}},{"cell_type":"code","source":"information = {'xmin': [], 'ymin': [], 'xmax': [], 'ymax': [], 'label': [], 'file': [], 'width': [], 'height': []}\n\nfor annotation in glob.glob(annotations_directory + '/*.xml'):\n    tree = ElementTree.parse(annotation)\n    \n    for element in tree.iter():\n        if 'size' in element.tag:\n            for attribute in list(element):\n                if 'width' in attribute.tag: \n                    width = int(round(float(attribute.text)))\n                if 'height' in attribute.tag:\n                    height = int(round(float(attribute.text)))    \n\n        if 'object' in element.tag:\n            for attribute in list(element):\n                \n                if 'name' in attribute.tag:\n                    name = attribute.text                 \n                    information['label'] += [name]\n                    information['width'] += [width]\n                    information['height'] += [height] \n                    information['file'] += [annotation.split('/')[-1][0:-4]] \n                            \n                if 'bndbox' in attribute.tag:\n                    for dimension in list(attribute):\n                        if 'xmin' in dimension.tag:\n                            xmin = int(round(float(dimension.text)))\n                            information['xmin'] += [xmin]\n                        if 'ymin' in dimension.tag:\n                            ymin = int(round(float(dimension.text)))\n                            information['ymin'] += [ymin]                                \n                        if 'xmax' in dimension.tag:\n                            xmax = int(round(float(dimension.text)))\n                            information['xmax'] += [xmax]                                \n                        if 'ymax' in dimension.tag:\n                            ymax = int(round(float(dimension.text)))\n                            information['ymax'] += [ymax]","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"annotations_info_df = pd.DataFrame(information)\nannotations_info_df.head(10)","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Add Annotation and Image File Names\nannotations_info_df['annotation_file'] = annotations_info_df['file'] + '.xml'\nannotations_info_df['image_file'] = annotations_info_df['file'] + '.png'\n\nannotations_info_df.loc[annotations_info_df['label'] == 'mask_weared_incorrect', 'label'] = 'mask_incorrectly_worn'","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"annotations_info_df","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Check If Label Is Right\n\nImage 737 (`maksssksksss737.png`) is labeled with several categories, we can simply check the actual image to see if the labels are right.","metadata":{}},{"cell_type":"code","source":"# Function to Show Actual Image\ndef render_image(image):\n    plt.figure(figsize = (12, 8))\n    plt.imshow(image)\n    plt.show()\n    \n# Function to Convert BGR to RGB\ndef convert_to_RGB(image):\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Image 737 File Name\nannotations_info_df['image_file'].iloc[0]","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Image 737 File Path\nimage_737_path = '../input/face-mask-detection/images/' + annotations_info_df['image_file'].iloc[0]\nimage_737_path","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Read Image 737 Using It's Path\nimage_737 = cv2.imread(image_737_path)\nimage_737","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Display The Image in RGB\nrender_image(convert_to_RGB(image_737))","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Image 737 Annotation\nannotation_737_path = '../input/face-mask-detection/annotations/' + annotations_info_df['annotation_file'].iloc[0]\nannotation_737_path","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Shape of Image 737\nimage_737.shape","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Crop Images\n\nAs there are multiple labels in an image (caused by more than 1 person in an image), we need to crop the image into several images that only consist of 1 person. We can use one of the images (ex: `image_737`) as our sample to make sure that we can crop images in a correct way.\n\nWe need xmin, ymin, xmax, and ymax values so that we can crop the image within the bounding box.","metadata":{}},{"cell_type":"code","source":"x = annotations_info_df['xmin'].iloc[0]\ny = annotations_info_df['ymin'].iloc[0]\nwidth = annotations_info_df['xmax'].iloc[0]\nheight = annotations_info_df['ymax'].iloc[0]\n\ncropped_737 = image_737[y:height, x:width]\nrender_image(cropped_737)","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now, we cropped the leftmost person in `image_737`. Displaying the image in RGB form.","metadata":{}},{"cell_type":"code","source":"render_image(convert_to_RGB(cropped_737))","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Now, we already know the way to crop a single image. We need to apply this to all images in the dataframe. So, there will be around 4072 cropped images so the **\"multiple label in an image\"** problem is solved.","metadata":{}},{"cell_type":"code","source":"len(annotations_info_df)","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"annotations_info_df.head(10)","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Create a New Directory For Cropped Images\n\nIn this section, we are going to crop all images and save them into a new directory.","metadata":{}},{"cell_type":"code","source":"!ls '../input/face-mask-detection'","metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"directory = 'cropped_images'\nparent_directory = '/kaggle/working'\npath = os.path.join(parent_directory, directory)\nos.mkdir(path)","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"!ls './'","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Adding Cropped Images Into New Directory","metadata":{}},{"cell_type":"code","source":"# Copy The File Name (Before appending with .png extension)\nannotations_info_df['cropped_image_file'] = annotations_info_df['file']\nannotations_info_df","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"After the `./cropped_images` directory is created, now we can insert all cropped images to that directory. We can simply run a loop to crop all images.","metadata":{}},{"cell_type":"code","source":"for i in range(len(annotations_info_df)):\n    # Get The File Path and Read The Image\n    image_filepath = '../input/face-mask-detection/images/' + annotations_info_df['image_file'].iloc[i]\n    image = cv2.imread(image_filepath)\n    \n    # Set The Cropped Image File Name\n    annotations_info_df['cropped_image_file'].iloc[i] = annotations_info_df['cropped_image_file'].iloc[i] + '-' + str(i) + '.png'\n    cropped_image_filename = annotations_info_df['cropped_image_file'].iloc[i]\n    \n    # Get The xmin, ymin, xmax, ymax Value (Bounding Box) to Crop Image\n    xmin = annotations_info_df['xmin'].iloc[i]\n    ymin = annotations_info_df['ymin'].iloc[i]\n    xmax = annotations_info_df['xmax'].iloc[i]\n    ymax = annotations_info_df['ymax'].iloc[i]\n\n    # Crop The Image Based on The Values Above\n    cropped_image = image[ymin:ymax, xmin:xmax]\n    \n    # Save Cropped Image\n    cropped_image_directory = os.path.join('./cropped_images', cropped_image_filename) \n    cv2.imwrite(cropped_image_directory, cropped_image)","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"annotations_info_df","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Check If Cropped Images Are Successfully Saved to Directory\n\nCropped images are saved into the `./cropped_images` directory.","metadata":{}},{"cell_type":"code","source":"cropped_images_files = !ls './cropped_images'\ncropped_images_files[:10]","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print('There are {} cropped images in total.'.format(len(cropped_images_files)))","metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Taking Image 737 again (the image in the zeroth index), and display the cropped image. ","metadata":{}},{"cell_type":"code","source":"# Image 737 File Name\nannotations_info_df['cropped_image_file'].iloc[0]","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Image 737 File Path\ncropped_737_0_path = './cropped_images/' + annotations_info_df['cropped_image_file'].iloc[0]\ncropped_737_0_path","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Read Image 737 Using It's Path\ncropped_737_0 = cv2.imread(cropped_737_0_path)\ncropped_737_0","metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Display Image 737 in RGB\nrender_image(convert_to_RGB(cropped_737_0))","metadata":{"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Sample Cropped Image Shape\ncropped_737_0.shape","metadata":{"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split\n\nTraining - 25% of the full dataset\nTesting - Rest 75% of the dataset","metadata":{}},{"cell_type":"code","source":"# Data Splitting\ntest_df = annotations_info_df[:800]\ntrain_df = annotations_info_df[800:]\n\n# Check The Shape of Splitted Data (Train and Test)\ntrain_df.shape, test_df.shape","metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Glimpse of Train Data\ntrain_df.head()","metadata":{"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Number of Categories / Labels\nclasses = list(train_df['label'].unique())","metadata":{"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\nIn Exploratory Data Analysis process, we only use the training dataset. \n* Classification whether the person in a image is \n1. with mask\n2. without mask\n3. wearing mask in incorrect way.","metadata":{}},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['file'] == 'maksssksksss139']['label'].unique()","metadata":{"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"image_139_path = '../input/face-mask-detection/images/maksssksksss139.png'\nimage_139 = cv2.imread(image_139_path)\nimage_139","metadata":{"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"image_139_rgb = convert_to_RGB(image_139)\nrender_image(image_139_rgb)","metadata":{"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"image_139_df = train_df[train_df['file'] == 'maksssksksss139']\nimage_139_df","metadata":{"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"with_mask_list, without_mask_list, incorrectly_worn_list = [], [], []\nfor i in range(len(image_139_df)):\n    bounding_box = [image_139_df['xmin'].iloc[i], image_139_df['ymin'].iloc[i],\n                    image_139_df['xmax'].iloc[i], image_139_df['ymax'].iloc[i]]\n    if image_139_df['label'].iloc[i] == 'with_mask':\n        with_mask_list.append(bounding_box)\n    elif image_139_df['label'].iloc[i] == 'without_mask':\n        without_mask_list.append(bounding_box)\n    else:\n        incorrectly_worn_list.append(bounding_box)\n        \nfound_objects_dict = {'With Mask': with_mask_list, \n                      'Without Mask': without_mask_list, \n                      'Incorrectly Worn': incorrectly_worn_list}\nfound_objects_dict","metadata":{"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"for key, value in found_objects_dict.items():\n    for i in range(len(value)):\n        color = (0, 255, 0) # green\n        text = 'Mask'\n        if key == 'Without Mask':\n            color = (255, 0, 0) # red\n            text = 'No Mask'\n        elif key == 'Incorrectly Worn':\n            color = (255, 255, 0) # yellow\n            text = 'Incorrect'\n        start_point = (value[i][0], value[i][1])\n        end_point = (value[i][2], value[i][3])\n        cv2.rectangle(image_139_rgb, start_point, end_point, color = color, thickness = 2)\n        cv2.putText(image_139_rgb, org = (value[i][0] - 8, value[i][1] - 3), text = text, \n                    fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.5, color = color)","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"render_image(image_139_rgb)","metadata":{"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"=====================","metadata":{}},{"cell_type":"code","source":"# Count Occurence of Labels\ntrain_df['label'].value_counts()","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"sorted_label_df = pd.DataFrame(train_df['label'].value_counts()).reset_index()\nsorted_label_df.rename(columns = {'index': 'label', 'label': 'count'}, inplace = True)\nsorted_label_df","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn')\nplt.figure(figsize = (8, 6))\nbarplot = sns.barplot(x = 'count', y = 'label', data = sorted_label_df, orient = 'horizontal', \n                      palette = ['green', 'red', 'yellow'])\nplt.title('Distribution of Labels', fontsize = 20, fontweight = 'bold')\nplt.xlabel('Count', fontsize = 15, fontweight = 'bold')\nplt.ylabel('Label', fontsize = 15, fontweight = 'bold')\n\nfor p in barplot.patches:\n    width = p.get_width()\n    percentage = round(width * 100 / sum(sorted_label_df['count']), 2)\n    plt.text(x = width + 15, y = p.get_y() + 0.55 * p.get_height(), s = f'{int(width)}\\n({percentage} %)')\n\nplt.show()","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"cropped_image_path = './cropped_images/' + train_df['cropped_image_file'].iloc[0]\ncropped_image = cv2.imread(cropped_image_path)\ncropped_image.shape","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"cropped_image.shape[0]","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"image_width = []\nimage_height = []\nfor i in range(len(train_df)):\n    cropped_image_path = './cropped_images/' + train_df['cropped_image_file'].iloc[i]\n    cropped_image = cv2.imread(cropped_image_path)\n    image_width.append(cropped_image.shape[0])\n    image_height.append(cropped_image.shape[1])","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"sns.histplot(image_width, kde = True)\nplt.title('Image Width Distribution', fontsize = 16, fontweight = 'bold')\nplt.xlabel('Image Width', fontweight = 'bold')\nplt.ylabel('Count', fontweight = 'bold')\nplt.show()","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"sns.histplot(image_height, kde = True)\nplt.title('Image Height Distribution', fontsize = 16, fontweight = 'bold')\nplt.xlabel('Image Height', fontweight = 'bold')\nplt.ylabel('Count', fontweight = 'bold')\nplt.show()","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"print('IMAGE WIDTH')\nprint(f'Min: {min(image_width)}')\nprint(f'Max: {max(image_width)}')\nprint(f'Mean: {np.mean(image_width)}')\nprint(f'Median: {np.median(image_width)}')\nprint('IMAGE HEIGHT')\nprint(f'Min: {min(image_height)}')\nprint(f'Max: {max(image_height)}')\nprint(f'Mean: {np.mean(image_height)}')\nprint(f'Median: {np.median(image_height)}')","metadata":{"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"image_target_size = (int(np.median(image_width)), int(np.median(image_height)))\nimage_target_size","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Image Data Generator","metadata":{}},{"cell_type":"code","source":"from keras_preprocessing.image import ImageDataGenerator\n\ntrain_image_generator = ImageDataGenerator(rescale = 1. / 255., validation_split = 0.25)\n\ntrain_generator = train_image_generator.flow_from_dataframe(\n    dataframe = train_df,\n    directory = './cropped_images',\n    x_col = 'cropped_image_file',\n    y_col = 'label',\n    subset = 'training',\n    batch_size = 32,\n    seed = 42,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = image_target_size\n)\n\nvalid_generator = train_image_generator.flow_from_dataframe(\n    dataframe = train_df,\n    directory = './cropped_images',\n    x_col = 'cropped_image_file',\n    y_col = 'label',\n    subset = 'validation',\n    batch_size = 32,\n    seed = 42,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = image_target_size\n)","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"test_image_generator = ImageDataGenerator(rescale = 1. / 255.)\n\ntest_generator = train_image_generator.flow_from_dataframe(\n    dataframe = test_df,\n    directory = './cropped_images',\n    x_col = 'cropped_image_file',\n    y_col = 'label',\n    batch_size = 32,\n    seed = 42,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = image_target_size\n)","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(train_generator)\nprint(valid_generator)\nprint(test_generator)","metadata":{"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"input_shape = [int(np.median(image_width)), int(np.median(image_height)), 3]","metadata":{"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"model_1 = keras.models.Sequential([\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu', \n                        input_shape = input_shape),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Flatten(),\n    keras.layers.Dense(units = len(classes), activation = 'softmax')\n])","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"model_1.compile(loss = 'categorical_crossentropy',\n                optimizer = keras.optimizers.Adam(),\n                metrics = ['accuracy', keras.metrics.Recall()])\n\nhistory_1 = model_1.fit(train_generator, epochs = 10, steps_per_epoch = len(train_generator), \n                        validation_data = valid_generator, validation_steps = len(valid_generator))","metadata":{"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"result_1 = pd.DataFrame(history_1.history)\nresult_1","metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"result_1.plot()","metadata":{"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def plot_line(result, ax, col, title, train_column, valid_column):\n    # Line Plot of Model Performance\n    ax[col].plot(result[train_column])\n    ax[col].plot(result[valid_column])\n    \n    # Title and Legend\n    ax[col].set_title(title, fontweight = 'bold')\n    ax[col].legend(['Train', 'Validation'])\n    \ndef plot_result(result, train_recall, valid_recall):\n    # Create a 1x3 Grid and Set Main Title\n    fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (17, 8))\n    fig.suptitle('Model Performance', fontsize = 20, fontweight = 'bold')\n    \n    # Visualization of Accuracy, Recall, and Loss\n    plot_line(result, ax, 0, 'Accuracy', 'accuracy', 'val_accuracy')\n    plot_line(result, ax, 1, 'Recall', train_recall, valid_recall)\n    plot_line(result, ax, 2, 'Loss', 'loss', 'val_loss')\n    plt.show()","metadata":{"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"plot_result(result_1, 'recall', 'val_recall')","metadata":{"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# Adding Early Stopping Callback","metadata":{}},{"cell_type":"code","source":"model_2 = keras.models.Sequential([\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu', input_shape = input_shape),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Flatten(),\n    keras.layers.Dense(units = len(classes), activation = 'softmax')\n])","metadata":{"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model_2.compile(loss = 'categorical_crossentropy',\n                optimizer = keras.optimizers.Adam(),\n                metrics = ['accuracy', keras.metrics.Recall()])\n\ncallbacks = [keras.callbacks.EarlyStopping()]\n\nhistory_2 = model_2.fit(train_generator, epochs = 100, steps_per_epoch = len(train_generator), \n                        validation_data = valid_generator, validation_steps = len(valid_generator),\n                        callbacks = callbacks)","metadata":{"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"result_2 = pd.DataFrame(history_2.history)\nresult_2","metadata":{"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"result_2.plot()","metadata":{"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"plot_result(result_2, 'recall_1', 'val_recall_1')","metadata":{"trusted":true},"execution_count":72,"outputs":[]}]}